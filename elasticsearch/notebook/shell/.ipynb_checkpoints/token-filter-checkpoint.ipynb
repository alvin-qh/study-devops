{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词过滤器接受来自分词器的结果流，可以在此基础上完成：\n",
    "- 修改分词结果（如改为小写字母）;\n",
    "- 删除词汇（如删除停止词）;\n",
    "- 添加词汇（如同义词）;\n",
    "\n",
    "Elasticsearch 有许多内置的过滤器，您可以使用它们来构建自定义文本分析器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 内置分词过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 撇号过滤器 (Apostrophe token filter)\n",
    "\n",
    "删除撇号后的所有字符，包括撇号本身。<br>\n",
    "\n",
    "这个过滤器包含在 Elasticsearch 内置的[土耳其语分析器](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#turkish-analyzer)中。它使用[Lucene撇号过滤器](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html)，是为土耳其语构建的。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"apostrophe\"\n",
    "    ],\n",
    "    \"text\": \"Istanbul\\'a veya Istanbul\\'dan\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_with_apostrophe_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"apostrophe\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"standard_with_apostrophe_analyzer\",\n",
    "    \"text\": \"Istanbul\\'a veya Istanbul\\'dan\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ASCII 折叠过滤器 (ASCII folding token filter)\n",
    "\n",
    "将不属于基本拉丁 Unicode 块(前 127 个 ASCII 字符)的字母、数字和符号转换为 ASCII 等效字符（如果存在的话）。例如，过滤器将`à`更改为`a`。<br>\n",
    "这个过滤器使用 Lucene 的 [ASCIIFoldingFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html)。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"asciifolding\"\n",
    "    ],\n",
    "    \"text\": \"açaí à la carte\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_with_asciifolding_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"asciifolding\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"standard_with_asciifolding_analyzer\",\n",
    "    \"text\": \"açaí à la carte\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 中日韩二元模型过滤器 (CJK bigram token filter)\n",
    "\n",
    "使用 CJK (中文、日文和韩文) 标记形成“二元模型”。<br>\n",
    "该过滤器包含在 Elasticsearch 的内置 [CJK 语言分析器中](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#cjk-analyzer)。它使用 Lucene 的 [CJKBigramFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html)。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"cjk_bigram\"\n",
    "    ],\n",
    "    \"text\": \"東京都は、日本の首都であり\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_with_cjk_bigram_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"cjk_bigram\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"standard_with_cjk_bigram_analyzer\",\n",
    "    \"text\": \"東京都は、日本の首都であり\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 中日韩字符宽度过滤器 (CJK width token filter)\n",
    "\n",
    "将中日韩文字的宽度差异正常化如下:\n",
    "\n",
    "- 将全宽度 ASCII 字符变体折叠成等效的基本拉丁字符;\n",
    "- 将半宽度片假名字符变体折叠成等效的假名字符;\n",
    "- 该过滤器包含在 Elasticsearch 的内置 [CJK 语言分析器](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#cjk-analyzer)中。它使用 Lucene 的 [CJKWidthFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"cjk_width\"\n",
    "    ],\n",
    "    \"text\": \"ｼｰｻｲﾄﾞﾗｲﾅｰ\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_with_cjk_width_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"cjk_width\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"standard_with_cjk_width_analyzer\",\n",
    "    \"text\": \"ｼｰｻｲﾄﾞﾗｲﾅｰ\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. 经典过滤器 (Classic token filter)\n",
    "\n",
    "对[“经典分词器”](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-classic-tokenizer.html)生成的“词汇”执行可选的后置处理。<br>\n",
    "这个过滤器将英语所有格 (`'s`) 从单词的末尾移除，并将缩略词中的点移除。它使用 Lucene 的 [ClassicFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html)。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"classic\"\n",
    "    ],\n",
    "    \"text\": \"The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_classic_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"classic\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"standard_classic_analyzer\",\n",
    "    \"text\": \"The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. 常用词二元模型过滤器 (Common grams token filter)\n",
    "\n",
    "为一组指定的常用词生成[二元模型](https://en.wikipedia.org/wiki/Bigram)组合。例如，可以指定`is`和`the`作为常用单词。然后，该过滤器将标记`[the, quick, fox, is, brown]`转换为`[the, the_quick, quick, fox, fox_is, is, is_brown, brown]`. \n",
    "\n",
    "如果不想完全忽略常见单词，可以使用 “common_grams 过滤器”来代替 “stop 过滤器”。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [CommonGramsFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html)。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"common_grams\",\n",
    "            \"common_words\": [\n",
    "                \"is\",\n",
    "                \"the\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"the quick fox is brown\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数\n",
    "    - `common_words`: (Required*, array of strings) 词表。过滤器为这些词标记生成二元词组。此参数或`common_words_path`参数都是必需的。\n",
    "    - `common_words_path`: (Required*, string) 包含令牌列表的文件的路径。过滤器为这些标记生成二元词组。此路径必须是绝对的或相对于配置位置的。该文件必须是 UTF-8 编码的。文件中的每个标记必须用一个换行符分隔。此参数或`common_words`参数都是必需的。\n",
    "    - `ignore_case`: (Optional, boolean) 如果为`true`，则普通单词匹配的匹配不区分大小写。默认值为`false`\n",
    "    - `query_mode`: (Optional, boolean) 如果为`true`，则过滤器从输出中排除以下词汇:\n",
    "        - 常用词汇的一元模型;\n",
    "        - 词汇后接普通单词的一元模型;\n",
    "        - 默认值为`false`。我们建议为文本分析器启用此参数。\n",
    "\n",
    "\n",
    "例如，可以启用此参数并将`is`和`the`指定为常用单词。此过滤器转换`[the, quick, fox, is, brown]`为`[the_quick, quick, fox_is, is_brown]`.\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"common_grams_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"common_grams_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"common_grams_filter\": {\n",
    "                    \"type\": \"common_grams\",\n",
    "                    \"common_words\": [\n",
    "                        \"a\", \n",
    "                        \"is\",\n",
    "                        \"the\"\n",
    "                    ],\n",
    "                    \"ignore_case\": true,\n",
    "                    \"query_mode\": true\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"common_grams_filter_analyzer\",\n",
    "    \"text\": \"the quick fox is brown\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. 条件过滤器 (Conditional token filter)\n",
    "\n",
    "将一组过滤器应用于与所提供的“条件脚本”相符的词汇中。例如：将`\"lowercase\"`过滤器应用于符合`\"token.getTerm().length() < 5\"`条件的词汇中。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [ConditionalTokenFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ConditionalTokenFilter.html)\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"condition\",\n",
    "            \"filter\": [\n",
    "                \"lowercase\"\n",
    "            ],\n",
    "            \"script\": {\n",
    "                \"source\": \"token.getTerm().length() < 5\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"THE QUICK BROWN FOX\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `filter`: (Required, array of token filters) 过滤器的数组。如果某词汇与参数中的“条件脚本”匹配，则按照提供的顺序将这些过滤器应用于令牌。在定义索引的字段映射关系时，这些过滤器可以包括在自定义过滤器中。\n",
    "    - `script`: (Required, [script object](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html)) Painless 脚本，表示一个条件。如果某词汇与此脚本匹配，则指定的过滤器将应用于该词汇。有关有效参数，请参阅[脚本参数](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html#_script_parameters)。只支持内联脚本，Painless 脚本在分析谓词上下文中执行，并且需要一个词汇属性。\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"condition_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"condition_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"condition_filter\": {\n",
    "                    \"type\": \"condition\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ],\n",
    "                    \"script\": {\n",
    "                        \"source\": \"token.getTerm().length() < 5\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"condition_filter_analyzer\",\n",
    "    \"text\": \"THE QUICK BROWN FOX\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. 数字过滤器 (Decimal digit token filter)\n",
    "\n",
    "将 Unicode Decimal_Number 常规类别中的所有数字转换为阿拉伯数字 (`0`-`9`)。例如，过滤器改变了孟加拉数字`৩`转为`3`。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [DecimalDigitFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/core/DecimalDigitFilter.html)\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"filter\": [\n",
    "        \"decimal_digit\"\n",
    "    ],\n",
    "    \"text\": \"१-one two-२ ३\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"decimal_digit_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"decimal_digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"decimal_digit_analyzer\",\n",
    "    \"text\": \"१-one two-२ ३\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. 分隔负载过滤器 (Delimited payload token filter)\n",
    "\n",
    "> 旧的`delimited_payload_filter`名称已弃用的，不应该与新索引一起使用。使用`delimited_payload`代替\n",
    "\n",
    "据指定的分隔符将词汇流分隔为词汇和负载。例如，可以使用带`|`分隔符的`delimited_payload`过滤器来将`the|1 quick|2 fox|3`拆分为`the`、`quick`和`fox`，它们的有效负载分别为`1`、`2`和`3`\n",
    "\n",
    "这个过滤器使用 Lucene 的 [DelimitedPayloadTokenFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.html)\n",
    "\n",
    "----\n",
    "\n",
    "#### 负载\n",
    "\n",
    "负载是用户定义的与词汇位置关联的二进制数据，并以base64编码的字节存储。<br>\n",
    "默认情况下，Elasticsearch 不存储词汇负载。要存储有效载荷，您必须:    \n",
    "- 将`term_vector`映射参数设置为`with_positions_payloads`或`with_positions_offsets_payloads`，用于存储任何有效载荷的字段。\n",
    "- 使用一个包含`delimited_payload`过滤器的索引分析器\n",
    "\n",
    "\n",
    "您可以使用[term vectors API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html)查看存储的有效载荷。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"filter\": [\n",
    "        \"delimited_payload\"\n",
    "    ],\n",
    "    \"text\": \"the|0 brown|10 fox|5 is|0 quick|10\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"delimited_payload_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"delimited_payload\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"delimited_payload_analyzer\",\n",
    "    \"text\": \"the|0 brown|10 fox|5 is|0 quick|10\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. 字典解码器过滤器 (Dictionary decompounder token filter)\n",
    "\n",
    "> 在大多数情况下，我们建议使用更快的`hyphenation_decompounder`令牌过滤器来代替这个过滤器。但是，您可以使用`dictionary_decompounder`过滤器来检查单词列表的质量，然后再在`hyphenation_decompounder`过滤器中实现它。\n",
    "\n",
    "使用指定的单词列表和暴力方法来查找复合词中的子单词。如果找到，这些子单词将包含在词汇输出中。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [DictionaryCompoundWordTokenFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.html)，它是为日耳曼语言构建的。\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"dictionary_decompounder\",\n",
    "            \"word_list\": [\n",
    "                \"Donau\", \n",
    "                \"dampf\",\n",
    "                \"meer\",\n",
    "                \"schiff\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"Donaudampfschiff\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `word_list`: (Required*, array of strings) 待查找的子单词列表。如果找到，子单词将包含在词汇输出中。必须指定此参数或`word_list_path`参数;\n",
    "    - `word_list_path`: (Required*, string) 待查找的子单词列表文件的路径。如果找到，子单词将包含在令牌输出中;\n",
    "    - `max_subword_size`: (Optional, integer) 最大子字符长度。较长的子单词标记被排除在输出之外。默认为`15`;\n",
    "    - `min_subword_size`: (Optional, integer) 最小子字符长度。较短的子单词标记被排除在输出之外。默认为`2`;\n",
    "    - `min_word_size`: (Optional, integer) 最小字符长度。较短的字标记被排除在输出之外。默认为`5`;\n",
    "    - `only_longest_match`: (Optional, boolean) 如果为`true`，则只包含最长的匹配子单词。默认值为`false`;\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"dictionary_decompounder_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"dictionary_decompounder_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"dictionary_decompounder_filter\": {\n",
    "                    \"type\": \"dictionary_decompounder\",\n",
    "                    \"word_list\": [\n",
    "                        \"Donau\", \n",
    "                        \"dampf\",\n",
    "                        \"meer\",\n",
    "                        \"schiff\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"dictionary_decompounder_filter_analyzer\",\n",
    "    \"text\": \"Donaudampfschiff\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11. 边缘 n-gram 标记过滤器 (Edge n-gram token filter)\n",
    "\n",
    "从词汇的开头形成指定长度的 “n-gram”。例如，您可以使用`edge_ngram`令牌过滤器将其快速更改为`qu`。如果没有自定义，过滤器默认情况下会创建`1`字符的边缘 \"n-gram\" 个字符。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [EdgeNGramTokenFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html)\n",
    "\n",
    "> `edge_ngram`过滤器类似于 [ngram token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html)。但是，`edge_ngram`只输出从标记开头开始的n个字符。这些边缘 \"n-grams\" 对于[“按类型搜索”](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-as-you-type.html)查询非常有用。\n",
    "\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"edge_ngram\",\n",
    "            \"min_gram\": 1,\n",
    "            \"max_gram\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"the quick brown fox jumps\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `max_gram`: (Required*, array of strings) 待查找的子单词列表。如果找到，子单词将包含在词汇输出中。必须指定此参数或`word_list_path`参数;\n",
    "    - `min_gram`: (Required*, string) 待查找的子单词列表文件的路径。如果找到，子单词将包含在令牌输出中;\n",
    "    - `side`: (Optional, integer) 最大子字符长度。较长的子单词标记被排除在输出之外。默认为`15`;\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"edge_ngram_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"edge_ngram_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"edge_ngram_filter\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 1,\n",
    "                    \"max_gram\": 2\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"edge_ngram_filter_analyzer\",\n",
    "    \"text\": \"the quick brown fox jumps\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12. 省略标记过滤器 (Elision token filter)\n",
    "\n",
    "删除令牌开头的指定[省略](https://en.wikipedia.org/wiki/Elision)。例如，您可以使用此筛选器将`l'avion`更改为`avion`。\n",
    "\n",
    "当未自定义时，过滤器默认删除以下法语部分: `l'`, `m'`, `t'`, `qu'`, `n'`, `s'`, `j'`, `d'`, `c'`, `jusqu'`, `quoiqu'`, `lorsqu'`, `puisqu'`\n",
    "\n",
    "这个过滤器的定制版本包含在 Elasticsearch 的几个内置语言文本分析器中:\n",
    "\n",
    "- [加泰罗尼亚](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#catalan-analyzer)\n",
    "- [法国](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#french-analyzer)\n",
    "- [爱尔兰](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#irish-analyzer)\n",
    "- [意大利](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#italian-analyzer)\n",
    "\n",
    "这个过滤器使用 Lucene 的 [ElisionFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html)\n",
    "\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"elision\"\n",
    "    ],\n",
    "    \"text\": \"j’examine près du wharf\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `max_gram`: (Required*, array of strings) 待查找的子单词列表。如果找到，子单词将包含在词汇输出中。必须指定此参数或`word_list_path`参数;\n",
    "    - `min_gram`: (Required*, string) 待查找的子单词列表文件的路径。如果找到，子单词将包含在令牌输出中;\n",
    "    - `side`: (Optional, integer) 最大子字符长度。较长的子单词标记被排除在输出之外。默认为`15`;\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"elision_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"elision\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"elision_filter_analyzer\",\n",
    "    \"text\": \"j’examine près du wharf\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13. 指纹标记过滤器 (Fingerprint token filter)\n",
    "\n",
    "从分词结果流中排序并删除重复的词汇，然后将该流连接到单个输出结果中。\n",
    "\n",
    "例如，这个过滤器将`[ the, fox, was, very, very, quick ]`词汇流更改如下:\n",
    "\n",
    "1. 将代币按字母顺序排列`[ fox, quick, the, very, very, was ]`;\n",
    "2. 删除重复的`very`词汇;\n",
    "3. 输出单个结果:`[fox quick the very was ]`;\n",
    "\n",
    "这个过滤器产生的输出标记对于指纹识别和聚集[OpenRefine](https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth#fingerprint)项目中描述的文本非常有用。\n",
    "\n",
    "这个过滤器使用 Lucene 的 [FingerprintFilter](https://lucene.apache.org/core/8_4_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/FingerprintFilter.html)\n",
    "\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"filter\": [\n",
    "        \"fingerprint\"\n",
    "    ],\n",
    "    \"text\": \"zebra jumps over resting resting dog\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `max_output_size`: (Optional, integer) 输出结果的最大字符长度，包括空格。默认为`255`。如果连接的令牌长度超过此值，则不会产生结果输出;\n",
    "    - `separator`: (Optional, string) 用于连接结果分词流的字符。默认为空格;\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"fingerprint_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"fingerprint_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"fingerprint_filter\": {\n",
    "                    \"type\": \"fingerprint\",\n",
    "                    \"separator\": \"-\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"fingerprint_filter_analyzer\",\n",
    "    \"text\": \"zebra jumps over resting resting dog\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14. 扁平化标记图过滤器 (Flatten graph token filter)\n",
    "\n",
    "将其它图标记过滤器产生的结果扁平化，例如：[synonym_graph (同义词图过滤器)](https://www.elastic.co/guide/en/elasticsearch/reference/current/token-graphs.html)或 [word_delimiter_graph](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html) 生成的标记图\n",
    "\n",
    "将包含多位置标记的标记图压扁，使其适合于索引。索引不支持包含多位置标记的标记图。\n",
    "\n",
    "> 扁平化图是一个有损过程。<br>\n",
    "> 如果可能，避免使用展平图过滤器。相反，只在搜索分析器中使用图标记过滤器。这样就不需要平坦图过滤器了。\n",
    "\n",
    "扁平化可以使下面的标记图\n",
    "![Token Graph](./assets/token-graph-dns-synonym-ex.svg)\n",
    "\n",
    "转换为<br>\n",
    "![Token Graph](./assets/token-graph-dns-invalid-ex.svg)\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"synonym_graph\",\n",
    "            \"synonyms\": [\n",
    "                \"dns, domain name system\"\n",
    "            ]\n",
    "        },\n",
    "        \"flatten_graph\"\n",
    "    ],\n",
    "    \"text\": \"domain name system is fragile\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `max_output_size`: (Optional, integer) 输出结果的最大字符长度，包括空格。默认为`255`。如果连接的令牌长度超过此值，则不会产生结果输出;\n",
    "    - `separator`: (Optional, string) 用于连接结果分词流的字符。默认为空格;\n",
    "\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"synonym_graph_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"synonym_graph_filter\",\n",
    "                        \"flatten_graph\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"synonym_graph_filter\": {\n",
    "                    \"type\": \"synonym_graph\",\n",
    "                    \"synonyms\": [\n",
    "                        \"dns, domain name system\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"synonym_graph_filter_analyzer\",\n",
    "    \"text\": \"domain name system is fragile\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15. Hunspell 令牌筛选器 (Hunspell token filter)\n",
    "\n",
    "基于词典提取词干的词汇过滤器。该过滤器从文件系统 (`<path.conf>/hunspell`) 选取 Hunspell 字典。每个字典都应使用区域代码 (例如`zh_CN`) 设置（语言）命名的自己的目录。此字典目录应保存单个`.aff`和一个或多个`.dic`文件（所有这些文件将自动选取）。<br>\n",
    "例如，假设使用默认的 Hunspell 位置，以下目录布局将定义`en_US`字典：\n",
    "```plain\n",
    "- conf\n",
    "    |-- hunspell\n",
    "    |    |-- en_US\n",
    "    |    |    |-- en_US.dic\n",
    "    |    |    |-- en_US.aff\n",
    "```\n",
    "每个字典都可以配置一个设置：\n",
    "- `ignore_case`: 如果为`true`，则字典匹配将不区分大小写（默认值为`false`）;\n",
    "\n",
    "此设置可以在`elasticsearch.yml`配置文件中全局配置:\n",
    "- `indices.analysis.hunspell.dictionary.ignore_case`\n",
    "\n",
    "或特定词典：\n",
    "- `indices.analysis.hunspell.dictionary.en_US.ignore_case`\n",
    "\n",
    "还可以包含在典目录下的`settings.yml`文件中（这将覆盖`elasticsearch.yml`中定义的设置）。\n",
    "\n",
    "\n",
    "- 过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        \"lowercase\",\n",
    "        {\n",
    "            \"type\": \"hunspell\",\n",
    "            \"locale\": \"en_US\",\n",
    "            \"dedup\": true\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数：\n",
    "    - `locale`: 此筛选器的区位设置。如果未设置，则改为使用`lang`或`language`参数——即至少设置这三个参数的其中一个;\n",
    "    - `dictionary`: 字典的名称。在通过`indices.analysis.hunspell.dictionary.location`之前配置 hunspell 字典的路径;\n",
    "    - `dedup`: 如果只返回唯一的词汇，则需要将此词设置为`true`。默认值为`true`;\n",
    "    - `longest_only`: 如果只应返回最长的词汇，则设置为`true`。默认值为`false`, 返回所有可能的结果;\n",
    "    \n",
    "> 与 snowball stemmers（基于算法）相反，这是一个基于字典查找的过滤器器，因此结果的质量取决于字典的质量。\n",
    "\n",
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"hunspell_filter_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"hunspell_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"hunspell_filter\": {\n",
    "                    \"type\": \"hunspell\",\n",
    "                    \"locale\": \"en_US\",\n",
    "                    \"dedup\": true\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"hunspell_filter_analyzer\",\n",
    "    \"text\": \"The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
