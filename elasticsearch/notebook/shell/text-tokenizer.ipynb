{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 内置分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 标准分词器（Standard Tokenizer）\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"standard_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"standard_tokenizer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"max_token_length\": 5\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"standard_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 字符分词器（Letter Tokenizer）\n",
    "\n",
    "字母记号赋予器在遇到非字母的字符时将文本分解成词汇。对于大多数欧洲语言来说，它的作用还算合理，但对于一些亚洲语言来说，就很糟糕了，因为亚洲语言单词之间没有空格。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"letter\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"letter_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"letter_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"letter_tokenizer\": {\n",
    "                    \"type\": \"letter\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"letter_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 小写字符分词器（Lowercase Tokenizer）\n",
    "\n",
    "功能上相当于 Letter 分词器与 Lowercase 过滤器的组合，但更有效，因为它在单次传递中执行这两个步骤。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"lowercase\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"lowercase_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"lowercase_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"lowercase_tokenizer\": {\n",
    "                    \"type\": \"lowercase\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"lowercase_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 空格分词器（Whitespace Tokenizer）\n",
    "\n",
    "每当遇到一个空白字符进行分词\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"whitespace_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"whitespace_tokenizer\": {\n",
    "                    \"type\": \"whitespace\",\n",
    "                    \"max_token_length\": 50\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"whitespace_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. URL 和 电子邮件 分词器（UAX URL Email Tokenizer）\n",
    "\n",
    "与标准的标记器类似，只是它将 url 和电子邮件地址识别为单个词汇。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"uax_url_email\",\n",
    "    \"text\": \"Email me at john.smith@global-international.com\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"uax_url_email_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"uax_url_email_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"uax_url_email_tokenizer\": {\n",
    "                    \"type\": \"uax_url_email\",\n",
    "                    \"max_token_length\": 120\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"uax_url_email_tokenizer_analyzer\",\n",
    "    \"text\": \"Email me at john.smith@global-international.com\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. 经典分词器（Classic Tokenizer）\n",
    "\n",
    "经典分词器器是基于语法的，适用于英语文档。该分词器对缩写词、公司名称、电子邮件地址和网址的特殊处理具有规则。然而，这些规则并不总是有效，分词器并不适用于除英语以外的大多数语言:\n",
    "- 主要依赖于标点符号分词，并去掉标点符号。但是没有空格的`.`被认为是词汇的一部分;\n",
    "- 它在连字符处拆分单词，除非词汇中有数字，在这种情况下，整个词汇被解释为产品编号，不进行拆分;\n",
    "- 它将电子邮件地址和互联网主机名识别为一个词汇;\n",
    "\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"classic\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"classic_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"classic_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"classic_tokenizer\": {\n",
    "                    \"type\": \"classic\",\n",
    "                    \"max_token_length\": 120\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"classic_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. 泰语分词器（Thai Tokenizer）\n",
    "\n",
    "泰语标记器将泰语文本分割成单词，使用Java中包含的泰语分割算法。其他语言的文本通常将被视为标准的记号赋予器。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"thai\",\n",
    "    \"text\": \"การที่ได้ต้องแสดงว่างานดี\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"thai_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"thai_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"thai_tokenizer\": {\n",
    "                    \"type\": \"thai\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"thai_tokenizer_analyzer\",\n",
    "    \"text\": \"การที่ได้ต้องแสดงว่างานดี\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. N-Gram 分词器\n",
    "\n",
    "类似于一个滑动窗口，在单词上移动一个指定长度的连续字符序列。它们对于查询不使用空格或具有长复合词的语言(如德语)非常有用。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"ngram\",\n",
    "    \"text\": \"Quick Fox\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `min_gram`: 分割的最小长度，默认为`1`;\n",
    "- `max_gram`: 分割的最大长度，默认为`2`;\n",
    "- `token_chars`: 应该包含在词汇中的字符类，这部分文本不进行拆分。默认为`[]`(保留所有字符)。字符类包括：\n",
    "    - `letter`: for example `a`, `b`, `ï` or `京`;\n",
    "    - `digit`: for example `3` or `7`;\n",
    "    - `whitespace`: for example \" \" or \"\\n\";\n",
    "    - `punctuation`: for example `!` or `\"`;\n",
    "    - `symbol`: for example `$` or `√`;\n",
    "    - `custom`: custom characters which need to be set using the custom_token_chars setting.\n",
    "    - `custom_token_chars`: Custom characters that should be treated as part of a token. For example, setting this to `+-_` will make the tokenizer treat the plus, minus and underscore sign as part of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"ngram_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"ngram_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"ngram_tokenizer\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 3,\n",
    "                    \"max_gram\": 3,\n",
    "                    \"token_chars\": [\n",
    "                        \"letter\",\n",
    "                        \"digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"ngram_tokenizer_analyzer\",\n",
    "    \"text\": \"2 Quick Foxes.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. Edge N-Gram 分词器\n",
    "\n",
    "在遇到指定字符列表时将文本分解为单词，然后发出每个单词的 N-Gram，其中 N-gram 的开头锚定到单词的开头。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"edge_ngram\",\n",
    "    \"text\": \"Quick Fox\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `min_gram`: 分割的最小长度，默认为`1`;\n",
    "- `max_gram`: 分割的最大长度，默认为`2`;\n",
    "- `token_chars`: 应该包含在词汇中的字符类，这部分文本不进行拆分。默认为`[]`(保留所有字符)。字符类包括：\n",
    "    - `letter`: for example `a`, `b`, `ï` or `京`;\n",
    "    - `digit`: for example `3` or `7`;\n",
    "    - `whitespace`: for example \" \" or \"\\n\";\n",
    "    - `punctuation`: for example `!` or `\"`;\n",
    "    - `symbol`: for example `$` or `√`;\n",
    "    - `custom`: custom characters which need to be set using the custom_token_chars setting.\n",
    "    - `custom_token_chars`: Custom characters that should be treated as part of a token. For example, setting this to `+-_` will make the tokenizer treat the plus, minus and underscore sign as part of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"edge_ngram_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"edge_ngram_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"edge_ngram_tokenizer\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 10,\n",
    "                    \"token_chars\": [\n",
    "                        \"letter\",\n",
    "                        \"digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"edge_ngram_tokenizer_analyzer\",\n",
    "    \"text\": \"2 Quick Foxes.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. 关键词分词器（Keyword Tokenizer）\n",
    "\n",
    "一个\"noop\"标记器，它接受它提供的任何文本，并输出与单个术语完全相同的文本。它可以与过滤器组合，使输出规范化，例如小写电子邮件地址。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"keyword\",\n",
    "    \"filter\": [\n",
    "        \"lowercase\"\n",
    "    ],\n",
    "    \"text\": \"New York\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `buffer_size`: 在一次传递中读取到缓冲区中的字符数，默认值为`256`。缓冲区会根据此大小增长，直到存储所有文本。建议不要更改此设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"keyword_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"keyword_tokenizer\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"keyword_tokenizer\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"buffer_size\": 256\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"keyword_tokenizer_analyzer\",\n",
    "    \"text\": \"New York\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. 正则分词器（Pattern Tokenizer）\n",
    "\n",
    "使用正则表达式在文本分隔符匹配时将文本拆分为词汇，或者将匹配的文本捕获为词汇。默认的正则表达式为`\\W+`\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"pattern\",\n",
    "    \"text\": \"The foo_bar_size default is 5.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `pattern`: 正则表达式，默认为`\\w+`;\n",
    "- `flags`: 标志位，可以用`|`组合，例如：`CASE_INSENSITIVE|COMMENTS`;\n",
    "- `group`: 指定分组编号为所提取词汇，默认为`-1`，表示全部拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"pattern_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"pattern_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"pattern_tokenizer\": {\n",
    "                    \"type\": \"pattern\",\n",
    "                    \"pattern\": \",\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"pattern_tokenizer_analyzer\",\n",
    "    \"text\": \"comma,separated,values\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
