{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分词器\n",
    "\n",
    "“文本分词器”接收字符流，将其分解为单个词汇，并输出词汇流。例如，[“空格分词器”](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-tokenizer.html)在看到任何空白时将文本分解为词汇。它会将文本 “Quick brown fox!” 转换成词汇流 `\\[Quick, brown, fox!\\]`。\n",
    "\n",
    "分词器还负责记录以下内容：\n",
    "- 每个词汇的顺序或位置（用于短语和单词邻近查询）;\n",
    "- 该词汇表示的原始单词的开始和结束字符偏移量（相对于原始文本，用于突出显示搜索片段）;\n",
    "- 词汇类型，产生的每个词汇的分类，如`<ALPHANUM>`， `<HANGUL>`，或`<NUM>`。更简单的分析程序只生成单词标记类型;\n",
    "\n",
    "Elasticsearch 有许多内建的分词器，可用于构建[自定义文本分析器](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 内置分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 标准分词器（Standard Tokenizer）\n",
    "\n",
    "标准分词器提供了基于语法的记号化（基于Unicode文本分割算法，如[Unicode标准附录#29](http://unicode.org/reports/tr29/)中所述），并且适用于大多数语言。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"standard_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"standard_tokenizer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"max_token_length\": 5\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"standard_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 字符分词器（Letter Tokenizer）\n",
    "\n",
    "字母记号赋予器在遇到非字母的字符时将文本分解成词汇。对于大多数欧洲语言来说，它的作用还算合理，但对于一些亚洲语言来说，就很糟糕了，因为亚洲语言单词之间没有空格。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"letter\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"letter_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"letter_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"letter_tokenizer\": {\n",
    "                    \"type\": \"letter\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"letter_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 小写字符分词器（Lowercase Tokenizer）\n",
    "\n",
    "功能上相当于 Letter 分词器与 Lowercase 过滤器的组合，但更有效，因为它在单次传递中执行这两个步骤。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"lowercase\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"lowercase_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"lowercase_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"lowercase_tokenizer\": {\n",
    "                    \"type\": \"lowercase\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"lowercase_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 空格分词器（Whitespace Tokenizer）\n",
    "\n",
    "每当遇到一个空白字符进行分词\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"whitespace_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"whitespace_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"whitespace_tokenizer\": {\n",
    "                    \"type\": \"whitespace\",\n",
    "                    \"max_token_length\": 50\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"whitespace_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. URL 和 电子邮件 分词器（UAX URL Email Tokenizer）\n",
    "\n",
    "与标准的标记器类似，只是它将 url 和电子邮件地址识别为单个词汇。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"uax_url_email\",\n",
    "    \"text\": \"Email me at john.smith@global-international.com\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"uax_url_email_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"uax_url_email_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"uax_url_email_tokenizer\": {\n",
    "                    \"type\": \"uax_url_email\",\n",
    "                    \"max_token_length\": 120\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"uax_url_email_tokenizer_analyzer\",\n",
    "    \"text\": \"Email me at john.smith@global-international.com\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. 经典分词器（Classic Tokenizer）\n",
    "\n",
    "经典分词器器是基于语法的，适用于英语文档。该分词器对缩写词、公司名称、电子邮件地址和网址的特殊处理具有规则。然而，这些规则并不总是有效，分词器并不适用于除英语以外的大多数语言:\n",
    "- 主要依赖于标点符号分词，并去掉标点符号。但是没有空格的`.`被认为是词汇的一部分;\n",
    "- 它在连字符处拆分单词，除非词汇中有数字，在这种情况下，整个词汇被解释为产品编号，不进行拆分;\n",
    "- 它将电子邮件地址和互联网主机名识别为一个词汇;\n",
    "\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=$'{\n",
    "    \"tokenizer\": \"classic\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数：\n",
    "- `max_token_length`: 最大分词长度。如果看到令牌超过此长度，则将其按 'max_token_length' 间隔分割。默认为 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"classic_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"classic_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"classic_tokenizer\": {\n",
    "                    \"type\": \"classic\",\n",
    "                    \"max_token_length\": 120\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text=$'{\n",
    "    \"analyzer\": \"classic_tokenizer_analyzer\",\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog\\'s bone.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. 泰语分词器（Thai Tokenizer）\n",
    "\n",
    "泰语标记器将泰语文本分割成单词，使用Java中包含的泰语分割算法。其他语言的文本通常将被视为标准的记号赋予器。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"thai\",\n",
    "    \"text\": \"การที่ได้ต้องแสดงว่างานดี\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"thai_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"thai_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"thai_tokenizer\": {\n",
    "                    \"type\": \"thai\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"thai_tokenizer_analyzer\",\n",
    "    \"text\": \"การที่ได้ต้องแสดงว่างานดี\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. N-Gram 分词器\n",
    "\n",
    "类似于一个滑动窗口，在单词上移动一个指定长度的连续字符序列。它们对于查询不使用空格或具有长复合词的语言(如德语)非常有用。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"ngram\",\n",
    "    \"text\": \"Quick Fox\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `min_gram`: 分割的最小长度，默认为`1`;\n",
    "- `max_gram`: 分割的最大长度，默认为`2`;\n",
    "- `token_chars`: 应该包含在词汇中的字符类，这部分文本不进行拆分。默认为`[]`(保留所有字符)。字符类包括：\n",
    "    - `letter`: for example `a`, `b`, `ï` or `京`;\n",
    "    - `digit`: for example `3` or `7`;\n",
    "    - `whitespace`: for example \" \" or \"\\n\";\n",
    "    - `punctuation`: for example `!` or `\"`;\n",
    "    - `symbol`: for example `$` or `√`;\n",
    "    - `custom`: custom characters which need to be set using the custom_token_chars setting.\n",
    "    - `custom_token_chars`: Custom characters that should be treated as part of a token. For example, setting this to `+-_` will make the tokenizer treat the plus, minus and underscore sign as part of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"ngram_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"ngram_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"ngram_tokenizer\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 3,\n",
    "                    \"max_gram\": 3,\n",
    "                    \"token_chars\": [\n",
    "                        \"letter\",\n",
    "                        \"digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"ngram_tokenizer_analyzer\",\n",
    "    \"text\": \"2 Quick Foxes.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. Edge N-Gram 分词器\n",
    "\n",
    "在遇到指定字符列表时将文本分解为单词，然后发出每个单词的 N-Gram，其中 N-gram 的开头锚定到单词的开头。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"edge_ngram\",\n",
    "    \"text\": \"Quick Fox\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `min_gram`: 分割的最小长度，默认为`1`;\n",
    "- `max_gram`: 分割的最大长度，默认为`2`;\n",
    "- `token_chars`: 应该包含在词汇中的字符类，这部分文本不进行拆分。默认为`[]`(保留所有字符)。字符类包括：\n",
    "    - `letter`: for example `a`, `b`, `ï` or `京`;\n",
    "    - `digit`: for example `3` or `7`;\n",
    "    - `whitespace`: for example \" \" or \"\\n\";\n",
    "    - `punctuation`: for example `!` or `\"`;\n",
    "    - `symbol`: for example `$` or `√`;\n",
    "    - `custom`: custom characters which need to be set using the custom_token_chars setting.\n",
    "    - `custom_token_chars`: Custom characters that should be treated as part of a token. For example, setting this to `+-_` will make the tokenizer treat the plus, minus and underscore sign as part of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"edge_ngram_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"edge_ngram_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"edge_ngram_tokenizer\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 10,\n",
    "                    \"token_chars\": [\n",
    "                        \"letter\",\n",
    "                        \"digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"edge_ngram_tokenizer_analyzer\",\n",
    "    \"text\": \"2 Quick Foxes.\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. 关键词分词器（Keyword Tokenizer）\n",
    "\n",
    "一个\"noop\"标记器，它接受它提供的任何文本，并输出与单个术语完全相同的文本。它可以与过滤器组合，使输出规范化，例如小写电子邮件地址。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"keyword\",\n",
    "    \"filter\": [\n",
    "        \"lowercase\"\n",
    "    ],\n",
    "    \"text\": \"New York\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "\n",
    "- `buffer_size`: 在一次传递中读取到缓冲区中的字符数，默认值为`256`。缓冲区会根据此大小增长，直到存储所有文本。建议不要更改此设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"keyword_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"keyword_tokenizer\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"keyword_tokenizer\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"buffer_size\": 256\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"keyword_tokenizer_analyzer\",\n",
    "    \"text\": \"New York\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. 正则分词器（Pattern Tokenizer and Simple Pattern Tokenizer）\n",
    "\n",
    "使用正则表达式在文本分隔符匹配时将文本拆分为词汇，或者将匹配的文本捕获为词汇。默认的正则表达式为`\\W+`\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"pattern\",\n",
    "    \"text\": \"The foo_bar_size default is 5.\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pattern Tokenizer 设置到索引\n",
    "\n",
    "参数:\n",
    "- `pattern`: 正则表达式，默认为`\\w+`;\n",
    "- `flags`: 标志位，可以用`|`组合，例如：`CASE_INSENSITIVE|COMMENTS`;\n",
    "- `group`: 指定分组编号为所提取词汇，默认为`-1`，表示全部拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"pattern_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"pattern_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"pattern_tokenizer\": {\n",
    "                    \"type\": \"pattern\",\n",
    "                    \"pattern\": \",\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"pattern_tokenizer_analyzer\",\n",
    "    \"text\": \"comma,separated,values\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple Pattern Tokenizer 设置到索引\n",
    "\n",
    "参数:\n",
    "- `pattern`: [Lucene 正则表达式](https://lucene.apache.org/core/8_4_0/core/org/apache/lucene/util/automaton/RegExp.html)，默认为空字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"simple_pattern_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"simple_pattern_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"simple_pattern_tokenizer\": {\n",
    "                    \"type\": \"simple_pattern\",\n",
    "                    \"pattern\": \"[0-9]{3}\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"simple_pattern_tokenizer_analyzer\",\n",
    "    \"text\": \"fd-786-335-514-x\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11. 字符组分词器（Char Group Tokenizer）\n",
    "\n",
    "遇到字符组中的字符时将文本分解为词汇（例如`,`）。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": {\n",
    "        \"type\": \"char_group\",\n",
    "        \"tokenize_on_chars\": [\n",
    "            \"whitespace\",\n",
    "            \"-\",\n",
    "            \"\\n\"\n",
    "        ]\n",
    "    },\n",
    "    \"text\": \"The QUICK brown-fox\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "- `tokenize_on_chars`: 包含用于对字符串进行标记的字符列表。每当遇到此列表中的字符时，就会分割一个新词汇。它可以接受单个字符(如`-`)，也可以接受字符组: `whitespace`, `letter`, `digit`, `punctuation` or `symbol`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"char_group_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"char_group_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"char_group_tokenizer\": {\n",
    "                    \"type\": \"char_group\",\n",
    "                    \"tokenize_on_chars\": [\n",
    "                        \"whitespace\",\n",
    "                        \"-\",\n",
    "                        \"\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"char_group_tokenizer_analyzer\",\n",
    "    \"text\": \"The QUICK brown-fox\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12. 字符组分词器（Simple Pattern Split Tokenizer）\n",
    "\n",
    "通过一个正则表达式将输入分割为多个词汇。它支持的正则表达式特性集比**模式分词器**更有限，但执行速度通常更快。<br>\n",
    "该分词器并不通过匹配项本身进行分词。要使用同一个受限制的正则表达式子集中的模式从匹配中生成词汇，请参阅[simple_pattern分词器](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simplepattern-tokenizer.html)。<br>\n",
    "这个分词器使用[Lucene正则表达式](http://lucene.apache.org/core/8_4_0/core/org/apache/lucene/util/automaton/RegExp.html)。有关受支持的特性和语法的说明，请参阅[正则表达式语法](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)。<br>\n",
    "默认模式是空字符串。此分词器应始终配置为非默认模式。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": {\n",
    "        \"type\": \"simple_pattern_split\",\n",
    "        \"pattern\": \"_\"\n",
    "    },\n",
    "    \"text\": \"an_underscored_phrase\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "- `pattern`: [Lucene正则表达式](https://lucene.apache.org/core/8_4_0/core/org/apache/lucene/util/automaton/RegExp.html)，默认为空字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"simple_pattern_split_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"simple_pattern_split_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"simple_pattern_split_tokenizer\": {\n",
    "                    \"type\": \"simple_pattern_split\",\n",
    "                    \"pattern\": \"_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"simple_pattern_split_tokenizer_analyzer\",\n",
    "    \"text\": \"an_underscored_phrase\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13. 路径分词器（Path Hierarchy Tokenizer）\n",
    "\n",
    "接受一个类似于文件系统路径的层次值，在路径分隔符上进行分割。\n",
    "\n",
    "- 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='{\n",
    "    \"tokenizer\": \"path_hierarchy\",\n",
    "    \"text\": \"/one/two/three\"\n",
    "}';\n",
    "\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/_analyze?pretty' -d \"$(echo $text)\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置到索引\n",
    "\n",
    "参数:\n",
    "- `delimiter`: 要用作路径分隔符的字符。默认为`/`;\n",
    "- `replacement`: 用于分隔符的可选替换字符。默认为`delimiter`;\n",
    "- `buffer_size`: 在每次遍历中读入缓冲区的字符数。默认为`1024`。缓冲区将以这个大小增长，直到所有文本都被使用完。建议不要更改此设置;\n",
    "- `reverse`: 如果设置为`true`，则以相反的顺序发出词汇。默认值为`false`;\n",
    "- `skip`: 要跳过的初始字符的数量。默认值为`0`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "echo -e \"* create index as: \";\n",
    "settings='{\n",
    "    \"settings\": {\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"path_hierarchy_tokenizer_analyzer\": {\n",
    "                    \"tokenizer\": \"path_hierarchy_tokenizer\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"path_hierarchy_tokenizer\": {\n",
    "                    \"type\": \"path_hierarchy\",\n",
    "                    \"reverse\": true,\n",
    "                    \"delimiter\": \"-\",\n",
    "                    \"replacement\": \"/\",\n",
    "                    \"skip\": 2\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X PUT 'http://localhost:9200/analyzer?pretty' -d \"$(echo $settings)\";\n",
    "\n",
    "# test index analyzer\n",
    "echo -e \"\\n* analyzed with index as: \";\n",
    "text='{\n",
    "    \"analyzer\": \"path_hierarchy_tokenizer_analyzer\",\n",
    "    \"text\": \"one-two-three-four-five\"\n",
    "}';\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X POST 'http://localhost:9200/analyzer/_analyze?pretty' -d \"$(echo $text)\";\n",
    "\n",
    "# delete index\n",
    "echo -e \"\\n* delete index as:\";\n",
    "curl -H 'Cache-Control: no-cache' -H 'Content-Type: application/json' \\\n",
    "     -X DELETE 'http://localhost:9200/analyzer?pretty';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
